<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <title>What is Artificial Intelligence (AI)? Tutorial, Meaning </title>
  <link rel="stylesheet" type="text/css" href="style.css" async="">
            <meta http-equiv="origin-trial" content="As0hBNJ8h++fNYlkq8cTye2qDLyom8NddByiVytXGGD0YVE+2CEuTCpqXMDxdhOMILKoaiaYifwEvCRlJ/9GcQ8AAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3MTk1MzI3OTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="AgRYsXo24ypxC89CJanC+JgEmraCCBebKl8ZmG7Tj5oJNx0cmH0NtNRZs3NB5ubhpbX/bIt7l2zJOSyO64NGmwMAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3MTk1MzI3OTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A/ERL66fN363FkXxgDc6F1+ucRUkAhjEca9W3la6xaLnD2Y1lABsqmdaJmPNaUKPKVBRpyMKEhXYl7rSvrQw+AkAAACNeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MTkzNTk5OTksImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A6OdGH3fVf4eKRDbXb4thXA4InNqDJDRhZ8U533U/roYjp4Yau0T3YSuc63vmAs/8ga1cD0E3A7LEq6AXk1uXgsAAACTeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MTkzNTk5OTksImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><script src="https://securepubads.g.doubleclick.net/pagead/managed/js/gpt/m202311090101/pubads_impl.js" async=""></script><script src="https://config.aps.amazon-adsystem.com/configs/2e7e1587-d92f-46dd-8721-80b53eccb87e" type="text/javascript" async="async"></script><style type="text/css">.vjs-youtube .vjs-iframe-blocker { display: none; }.vjs-youtube.vjs-user-inactive .vjs-iframe-blocker { display: block; }.vjs-youtube .vjs-poster { background-size: cover; }.vjs-youtube-mobile .vjs-big-play-button { display: none; }</style><style type="text/css">
   *,
*::before,
*::after {
	box-sizing: border-box;
}

* {
	margin: 0;
	padding: 0;
}

body {
	line-height: 1.5;
	font-family: 'Poppins', sans-serif;
}

.footer {
	background-color: #151515;
	padding: 80px 0;
}

.container {
	max-width: 1170px;
	margin: auto;
}

.row {
	display: flex;
	flex-wrap: wrap;
}

ul {
	list-style: none;
}

.footer-col {
	width: 25%;
	padding: 0 15px;
}

.footer-col h4 {
	font-size: 18px;
	color: #FFF;
	text-transform: capitalize;
	margin-bottom: 35px;
	font-weight: 500;
	position: relative;
}

.footer-col h4::before {
	content: "";
	position: absolute;
	left: 0;
	bottom: -10px;
	background-color: #E91E63;
	width: 50px;
	height: 2px;
}

.footer-col ul li:not(:last-child) {
	margin-bottom: 10px;
}

.footer-col ul li a {
	color: #DDD;
	display: block;
	font-size: 1rem;
	font-weight: 300;
	text-transform: capitalize;
	text-decoration: none;
	transition: all 0.3s ease;
}

.footer-col ul li a:hover {
	color: #FFF;
	padding-left: 7px;
}

.footer-col .social-links a {
	color: #FFF;
	background-color: rgba(255, 255, 255, 0.2);
	display: inline;
	height: 100px;
	width: 100px;
	border-radius: 100%;
	text-align: center;
	margin: 0 10px 10px 0;
	line-height: 100px;
	transition: all 0.5s ease;
}

.footer-col .social-links a:hover {
	color: #151515;
	background-color: #FFF;
}

@media(max-width: 767px) {
	.footer-col {
		width: 50%;
		margin-bottom: 30px;
	}
}

@media(max-width: 574px) {
	.footer-col {
		width: 100%;
	}
}
  body {
    background-image: url(photo/cc.avif);
    background-size: cover;
    background-position: center;
    background-repeat: no-repeat;
  }
  
  header {
    position: relative;
    background: transparent;
    padding: 1rem 0;
    z-index: 999;
  }
  
  .header-container {
    display: flex;
    align-items: center;
    justify-content: space-between;
    width: min(90%, 800px);
    margin-inline: auto;
    color: #000;
  }
  
  nav ul {
    display: flex;
    list-style: none;
    gap: 2rem;
  }
  
  nav a {
    text-decoration: none;
    color: #000;
  }
  
  nav a:hover {
    color: #4FC3F7;
  }
  
  .hamburger {
    display: none;
    cursor: pointer;
  }
  
  @media (max-width: 600px) {
    
    .toggle {
      transition: ease-in-out 550ms;
      transform: translate(0px);
      opacity: 1;
      display: block;
    }
    
    .header-container {
      width: 100%;
      padding: 0 1rem;
    }
    
    nav ul {
      flex-direction: column;
      position: absolute;
      top: 100%;
      left: 0;
      width: 100%;
      background: forestgreen;
      backdrop-filter: blur(10px);
      transform: translateX(-500px);
      opacity: 0;
    }
    
    nav li {
      padding: 1rem;
      cursor: pointer;
    }
    
    nav li:hover {
      background: forestgreen;
    }
    
    .hamburger {
      display: block; 
    }
  }
  div.onlycontent {
      margin-left: 10px;
      margin-top: 15px;
      margin-right: 4px;
      width: 58%;
      float: left;
      padding: 10px 15px;
      background :none;
    }
    #city td {
      color: black;
      line-height: 1.7;
    }
    #city li {
      line-height: 25px;
      color: black;
      margin-top: 4px;
    }
        </style>
       <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin="anonymous" referrerpolicy="no-referrer">
       <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" integrity="sha512-W/zrbCncQnky/EzL+/AYwTtosvrM+YG/V6piQLSe2HuKS6cmbw89kjYkp3tWFn1dkWV7L1ruvJyKbLz73Vlgfg==" crossorigin="anonymous" referrerpolicy="no-referrer">
       
       </head><body>
        <header>
          <div class="header-container">
            <span><img src="photo/logo1.png" style="height: 50px;width: 70px;overflow: hidden;backface-visibility: hidden;"></span>
            <nav id="nav">
              <b><i><ul id="nav-ul">
                <li><a href="nigam1.html">Home</a></li>
                <li><a href="about.html">About</a></li>
                <li><a href="AIservices.html">Service</a></li>
                <li><a href="contact.html">Contact</a></li>
              </ul></i></b>
            </nav>
            <span class="hamburger" id="button"><i class="fa-solid fa-bars"></i></span>
          </div>
        </header>
      <!-- Code injected by live-server -->
  <script>
    // <![CDATA[  <-- For SVG support
    if ('WebSocket' in window) {
      (function () {
        function refreshCSS() {
          var sheets = [].slice.call(document.getElementsByTagName("link"));
          var head = document.getElementsByTagName("head")[0];
          for (var i = 0; i < sheets.length; ++i) {
            var elem = sheets[i];
            var parent = elem.parentElement || head;
            parent.removeChild(elem);
            var rel = elem.rel;
            if (elem.href && typeof rel != "string" || rel.length == 0 || rel.toLowerCase() == "stylesheet") {
              var url = elem.href.replace(/(&|\?)_cacheOverride=\d+/, '');
              elem.href = url + (url.indexOf('?') >= 0 ? '&' : '?') + '_cacheOverride=' + (new Date().valueOf());
            }
            parent.appendChild(elem);
          }
        }
        var protocol = window.location.protocol === 'http:' ? 'ws://' : 'wss://';
        var address = protocol + window.location.host + window.location.pathname + '/ws';
        var socket = new WebSocket(address);
        socket.onmessage = function (msg) {
          if (msg.data == 'reload') window.location.reload();
          else if (msg.data == 'refreshcss') refreshCSS();
        };
        if (sessionStorage && !sessionStorage.getItem('IsThisFirstTime_Log_From_LiveServer')) {
          console.log('Live reload enabled.');
          sessionStorage.setItem('IsThisFirstTime_Log_From_LiveServer', true);
        }
      })();
    }
    else {
      console.error('Upgrade your browser. This Browser is NOT supported WebSocket for Live-Reloading.');
    }
    // ]]>
  </script>
  
      
      
      
     
      <div class="mobilemenu" style="clear:both">
      
      </div>
      <div id="menu">
      <div class="leftmenu2">
      <h2 class="spanh2"><span class="spanh2">Artificial Intelligence</span></h2>
      </div>
      <div class="leftmenu">
      <a href="nigam1.html" style="color: black;"><strong>Artificial Intelligence (AI)</strong> </a>
      <a href="nigam2.html">Applications of AI</a>
      <a href="nigam3.html">History of AI</a>
      <a href="nigam4.html">Types of AI</a>
      </div>
      <div class="leftmenu2">
      <h2 class="spanh2"><span class="spanh2">Intelligent Agent</span></h2>
      </div>
      <div class="leftmenu">
      <a href="nigam5.html">Types of Agents</a>
      <a href="nigam6.html">Intelligent Agent</a>
      <a href="nigam7.html">Agent Environment</a>
      <a href="nigam8.html">Turing Test in AI</a>
      </div>
      <div class="leftmenu2">
      <h2 class="spanh2"><span class="spanh2">Problem-solving</span></h2>
      </div>
      <div class="leftmenu">
      <a href="nigam9.html">Search Algorithms</a>
      <a href="nigam10.html">Uninformed Search Algorithm</a>
      <a href="nigam11.html">Informed Search Algorithms</a>
      <a href="nigam12.html">Hill Climbing Algorithm</a>
      <a href="nigam13.html">Means-Ends Analysis</a>
      </div>
      <div class="leftmenu2">
      <h2 class="spanh2"><span class="spanh2">Adversarial Search</span></h2>
      </div>
      <div class="leftmenu">
      <a href="nigam14.html">Adversarial search</a>
      <a href="nigam15.html">Minimax Algorithm</a>
      <a href="nigam16.html">Alpha-Beta Pruning</a>
      </div>
      <div class="leftmenu2">
      <h2 class="spanh2"><span class="spanh2">Knowledge Represent</span></h2>
      </div>
      <div class="leftmenu">
      <a href="nigam17.html">Knowledge Based Agent</a>
      <a href="nigam18.html">Knowledge Representation</a>
      <a href="nigam19.html">Knowledge Representation Techniques</a>
      <a href="nigam20.html">Propositional Logic</a>
      <a href="nigam21.html">Rules of Inference</a>
      <a href="nigam22.html">The Wumpus world</a>
      <a href="nigam23.html">knowledge-base for Wumpus World</a>
      <a href="nigam24.html">First-order logic</a>
      <a href="nigam25.html">Knowledge Engineering in FOL</a>
      <a href="nigam26.html">Inference in First-Order Logic</a>
      <a href="nigam27.html">Unification in FOL</a>
      <a href="nigam28.html">Resolution in FOL</a>
      <a href="nigam29.html">Forward Chaining and backward chaining</a>
      <a href="nigam30.html">Backward Chaining vs Forward Chaining</a>
      <a href="nigam31.html">Reasoning in AI</a>
      <a href="nigam32.html">Inductive vs. Deductive reasoning</a>
      </div>
      <div class="leftmenu2">
      <h2 class="spanh2"><span class="spanh2">Uncertain Knowledge R.</span></h2>
      </div>
      <div class="leftmenu">
      <a href="nigam33.html">Probabilistic Reasoning in AI</a>
      <a href="nigam34.html">Bayes theorem in AI</a>
      <a href="nigam35.html">Bayesian Belief Network</a>
      </div>
      <div class="leftmenu2">
      <h2 class="spanh2"><span class="spanh2">Misc</span></h2>
      </div>
      <div class="leftmenu">
      <a href="nigam36.html">Examples of AI</a>
      <a href="nigam37.html">AI Essay</a>
      <a href="nigam38.html">AI in Healthcare</a>
      <a href="nigam39.html">Artificial Intelligence in Education</a>
      <a href="nigam40.html">Artificial Intelligence in Agriculture</a>
      <a href="nigam41.html">Engineering Applications of AI</a>
      <a href="nigam42.html">Advantages &amp; Disadvantages of AI</a>
      <a href="nigam43.html">Robotics and AI</a>
      <a href="nigam44.html">Future of AI</a>
      <a href="nigam45.html">Languages used in AI</a>
      <a href="nigam46.html">Approaches to AI Learning</a>
      <a href="nigam47.html">Scope of AI</a>
      <a href="nigam48.html">Agents in AI</a>
      <a href="nigam49.html">Artificial Intelligence Jobs</a>
      <a href="nigam50.html">Amazon CloudFront</a>
      <a href="nigam51.html">Goals of Artificial Intelligence</a>
      <a href="nigam52.html">Can Artificial Intelligence replace Human Intelligence</a>
      <a href="nigam53.html">Importance of Artificial Intelligence</a>
      <a href="nigam54.html">Artificial Intelligence Stock in India</a>
      <a href="nigam55.html">How to Use Artificial Intelligence in Marketing</a>
      <a href="nigam56.html">Artificial Intelligence in Business</a>
      <a href="nigam57.html">Companies Working on Artificial Intelligence</a>
      <a href="nigam58.html">Artificial Intelligence Future Ideas</a>
      <a href="nigam59.html">Government Jobs in Artificial Intelligence in India</a>
      <a href="nigam60.html">What is the Role of Planning in Artificial Intelligence</a>
      <a href="nigam61.html">AI as a Service</a>
      <a href="nigam62.html">AI in Banking</a>
      <a href="nigam63.html">AI Tools</a>
      <a href="nigam64.html">Cognitive AI</a>
      <a href="nigam65.html">Introduction of Seaborn</a>
      <a href="nigam66.html">Natural Language ToolKit (NLTK)</a>
      <a href="nigam67.html">Best books for ML</a>
      <a href="nigam68.html">AI companies of India will lead in 2022</a>
      <a href="nigam69.html">Constraint Satisfaction Problems in Artificial Intelligence</a>
      <a href="nigam70.html">How artificial intelligence will change the future</a>
      <a href="nigam71.html">Problem Solving Techniques in AI</a>
      <a href="nigam72.html">AI in Manufacturing Industry</a>
      <a href="nigam73.html">Artificial Intelligence in Automotive Industry</a>
      <a href="nigam74.html">Artificial Intelligence in Civil Engineering</a>
      <a href="nigam75.html">Artificial Intelligence in Gaming Industry</a>
      <a href="nigam76.html">Artificial Intelligence in HR</a>
      <a href="nigam77.html">Artificial Intelligence in Medicine</a>
      <a href="nigam78.html">PhD in Artificial Intelligence</a>
      <a href="nigam79.html">Activation Functions in Neural Networks</a>
      <a href="nigam80.html">Boston Housing Kaggle Challenge with Linear Regression</a>
      <a href="nigam81.html">What are OpenAI and ChatGPT</a>
      <a href="nigam82.html">Chatbot vs. Conversational AI</a>
      <a href="nigam83.html">Iterative Deepening A* Algorithm (IDA*)</a>
      <a href="nigam84.html">Iterative Deepening Search (IDS) or Iterative Deepening Depth First Search (IDDFS)</a>
      </div>
      <div class="leftmenu2">
      <h2 class="spanh2"><span class="spanh2">Subsets of AI</span></h2>
      </div>
      <div class="leftmenu">
      <a href="nigam85.html">Subsets of AI</a>
      <a href="nigam86.html">Expert Systems</a>
      <a href="nigam87.html">Machine Learning Tutorial</a>
      <a href="nigam88.html">NLP Tutorial</a>
      </div>
      <div class="leftmenu2">
      <h2 class="spanh2"><span class="spanh2">Artificial Intelligence MCQ</span></h2>
      </div>
      <div class="leftmenu">
      <a href="nigam89.html">Artificial Intelligence MCQ</a>
      </div>
      <div class="leftmenu2">
      <h2 class="spanh2"><span class="spanh2">Related Tutorials</span></h2>
      </div>
      <div class="leftmenu">
      <a href="nigam92.html">Data Science Tutorial</a>
      <a href="nigam93.html">Reinforcement Learning</a>
      </div>
       
  </div>
  
 <div class="onlycontent">
    <div class="onlycontentad">
    </div>
    <div class="onlycontentinner">
    <div id="city">
    <table>
    <tbody><tr><td>
     <b><i><h1 class="h1">Reinforcement Learning Tutorial</h1>
    <img src="https://static.javatpoint.com/tutorial/reinforcement-learning/images/reinforcement-learning-tutorial.png" class="imageright" alt="Reinforcement Learning Tutorial">
    <p>Our Reinforcement learning tutorial will give you a complete overview of reinforcement learning, including MDP and Q-learning. In RL tutorial, you will learn the below topics:</p>
    <ul class="points">
    <li><a href="#What">What is Reinforcement Learning?</a></li>
    <li><a href="#Terms">Terms used in Reinforcement Learning.</a></li>
    <li><a href="#Key-features">Key features of Reinforcement Learning.</a></li>
    <li><a href="#Elements">Elements of Reinforcement Learning.</a></li>
    <li><a href="#Approaches">Approaches to implementing Reinforcement Learning.</a></li>
    <li><a href="#Work">How does Reinforcement Learning Work?</a></li>
    <li><a href="#Bellman">The Bellman Equation.</a></li>
    <li><a href="#Types">Types of Reinforcement Learning.</a></li>
    <li><a href="#Algorithm">Reinforcement Learning Algorithm.</a></li>
    <li><a href="#Markov">Markov Decision Process.</a></li>
    <li><a href="#Q-Learning">What is Q-Learning?</a></li>
    <li><a href="#Difference">Difference between Supervised Learning and Reinforcement Learning.</a></li>
    <li><a href="#Applications">Applications of Reinforcement Learning.</a></li>
    <li><a href="#Conclusion">Conclusion.</a></li>
    </ul>
    <hr>
    <h2 id="What" class="h2">What is Reinforcement Learning?</h2>
    <ul class="points">
    <li>Reinforcement Learning is a feedback-based Machine learning technique in which an agent learns to behave in an environment by performing the actions and seeing the results of actions. For each good action, the agent gets positive feedback, and for each bad action, the agent gets negative feedback or penalty.</li>
    <li>In Reinforcement Learning, the agent learns automatically using feedbacks without any labeled data, unlike <a href="https://www.javatpoint.com/supervised-machine-learning">supervised learning.</a></li>
    <li>Since there is no labeled data, so the agent is bound to learn by its experience only.</li>
    <li>RL solves a specific type of problem where decision making is sequential, and the goal is long-term, such as <strong>game-playing, robotics</strong>, etc.</li>
    <li>The agent interacts with the environment and explores it by itself. The primary goal of an agent in reinforcement learning is to improve the performance by getting the maximum positive rewards.</li>
    <li>The agent learns with the process of hit and trial, and based on the experience, it learns to perform the task in a better way. Hence, we can say that <strong><em>"Reinforcement learning is a type of machine learning method where an intelligent agent (computer program) interacts with the environment and learns to act within that."</em></strong> How a Robotic dog learns the movement of his arms is an example of Reinforcement learning.</li>
    <li>It is a core part of <a href="https://www.javatpoint.com/artificial-intelligence-tutorial">Artificial intelligence</a>, and all <a href="https://www.javatpoint.com/agents-in-ai">AI agent</a> works on the concept of reinforcement learning. Here we do not need to pre-program the agent, as it learns from its own experience without any human intervention.</li>
    <li><strong>Example:</strong> Suppose there is an AI agent present within a maze environment, and his goal is to find the diamond. The agent interacts with the environment by performing some actions, and based on those actions, the state of the agent gets changed, and it also receives a reward or penalty as feedback.</li>
    <li>The agent continues doing these three things (<strong>take action, change state/remain in the same state, and get feedback</strong>), and by doing these actions, he learns and explores the environment.</li>
    <li>The agent learns that what actions lead to positive feedback or rewards and what actions lead to negative feedback penalty. As a positive reward, the agent gets a positive point, and as a penalty, it gets a negative point.</li>
    </ul>
    <img src="https://static.javatpoint.com/tutorial/reinforcement-learning/images/what-is-reinforcement-learning.png" alt="What is Reinforcement Learning">
    <hr>
    <h2 id="Terms" class="h2">Terms used in Reinforcement Learning</h2>
    <ul class="points">
    <li><strong>Agent():</strong> An entity that can perceive/explore the environment and act upon it. </li>
    <li><strong>Environment():</strong> A situation in which an agent is present or surrounded by. In RL, we assume the stochastic environment, which means it is random in nature.</li>
    <li><strong>Action():</strong> Actions are the moves taken by an agent within the environment. </li>
    <li><strong>State():</strong> State is a situation returned by the environment after each action taken by the agent. </li>
    <li><strong>Reward():</strong> A feedback returned to the agent from the environment to evaluate the action of the agent. </li>
    <li><strong>Policy():</strong> Policy is a strategy applied by the agent for the next action based on the current state.</li>
    <li><strong>Value():</strong> It is expected long-term retuned with the discount factor and opposite to the short-term reward.</li>
    <li><strong>Q-value():</strong> It is mostly similar to the value, but it takes one additional parameter as a current action (a).</li>
    </ul>
    <hr>
    <h2 id="Key-Features" class="h2">Key Features of Reinforcement Learning</h2>
    <ul class="points">
    <li>In RL, the agent is not instructed about the environment and what actions need to be taken.</li>
    <li>It is based on the hit and trial process.</li>
    <li>The agent takes the next action and changes states according to the feedback of the previous action.</li>
    <li>The agent may get a delayed reward. </li>
    <li>The environment is stochastic, and the agent needs to explore it to reach to get the maximum positive rewards. </li>
    </ul>
    <hr>
    <h2 id="Approaches" class="h2">Approaches to implement Reinforcement Learning</h2>
    <p>There are mainly three ways to implement reinforcement-learning in ML, which are:</p>
    <ol class="points">
    <li><strong>Value-based:</strong><br>
    The value-based approach is about to find the optimal value function, which is the maximum value at a state under any policy. Therefore, the agent expects the long-term return at any state(s) under policy π.</li>
    <li><strong>Policy-based:</strong><br>
    Policy-based approach is to find the optimal policy for the maximum future rewards without using the value function. In this approach, the agent tries to apply such a policy that the action performed in each step helps to maximize the future reward.<br>
    The policy-based approach has mainly two types of policy:
    <ul class="points">
    <li><strong>Deterministic:</strong> The same action is produced by the policy (π) at any state. </li>
    <li><strong>Stochastic:</strong> In this policy, probability determines the produced action. </li>
    </ul></li>
    <li><strong>Model-based:</strong> In the model-based approach, a virtual model is created for the environment, and the agent explores that environment to learn it. There is no particular solution or algorithm for this approach because the model representation is different for each environment.</li>
    </ol>
    <hr>
    <div id="a9d156a7-075f-4e17-b533-47690b1b56b2" data-section="a9d156a7-075f-4e17-b533-47690b1b56b2" class="_ap_apex_ad" data-xpath="#city > table:eq(0) > tbody:eq(0) > tr:eq(0) > td:eq(0) > h2:eq(4)" data-section-id="" data-ap-network="adpTags" data-render-time="1700496564673" data-refresh-time="1700496660670" data-timeout="6518" style="display: block; clear: both; text-align: center; margin: 10px auto; width: 0px; height: 0px; overflow: hidden; visibility: hidden;"><div id="ADP_37780_728X280_a9d156a7-075f-4e17-b533-47690b1b56b2" style="text-align: center; margin: 0 auto;" data-google-query-id="CJP279z70oIDFRhkaAodsxAIJw">
    <script type="text/javascript">
    window.adpushup.adpTags.que.push(function(){
    window.adpushup.adpTags.display("ADP_37780_728X280_a9d156a7-075f-4e17-b533-47690b1b56b2");
    });
    </script>
    <div id="google_ads_iframe_/103512698,22511567001/22987847942_0__container__" style="border: 0pt none; width: 728px; height: 0px;"></div></div></div><h2 id="Elements" class="h2">Elements of Reinforcement Learning</h2>
    <p>There are four main elements of Reinforcement Learning, which are given below:</p>
    <ol class="points">
    <li>Policy</li>
    <li>Reward Signal</li>
    <li>Value Function</li>
    <li>Model of the environment</li>
    </ol>
    <p><strong>1) Policy:</strong> A policy can be defined as a way how an agent behaves at a given time. It maps the perceived states of the environment to the actions taken on those states. A policy is the core element of the RL as it alone can define the behavior of the agent. In some cases, it may be a simple function or a lookup table, whereas, for other cases, it may involve general computation as a search process. It could be deterministic or a stochastic policy: </p>
    <p><strong>For deterministic policy: a = π(s)</strong><br>
    <strong>For stochastic policy: π(a | s) = P[At =a | St = s]</strong></p>
    <p><strong>2) Reward Signal:</strong> The goal of reinforcement learning is defined by the reward signal. At each state, the environment sends an immediate signal to the learning agent, and this signal is known as a <strong>reward signal</strong>. These rewards are given according to the good and bad actions taken by the agent. The agent's main objective is to maximize the total number of rewards for good actions. The reward signal can change the policy, such as if an action selected by the agent leads to low reward, then the policy may change to select other actions in the future.</p>
    <p><strong>3) Value Function:</strong> The value function gives information about how good the situation and action are and how much reward an agent can expect. A reward indicates the <strong>immediate signal for each good and bad action</strong>, whereas a value function specifies <strong>the good state and action for the future</strong>. The value function depends on the reward as, without reward, there could be no value. The goal of estimating values is to achieve more rewards.</p>
    <p><strong>4) Model:</strong> The last element of reinforcement learning is the model, which mimics the behavior of the environment. With the help of the model, one can make inferences about how the environment will behave. Such as, if a state and an action are given, then a model can predict the next state and reward.</p>
    <p>The model is used for planning, which means it provides a way to take a course of action by considering all future situations before actually experiencing those situations. The approaches for solving the RL problems <strong>with the help of the model</strong> are termed as the <strong>model-based approach</strong>. Comparatively, an approach <strong>without using a model</strong> is called a <strong>model-free approach</strong>.</p>
    <hr>
    <h2 id="Work" class="h2">How does Reinforcement Learning Work?</h2>
    <p>To understand the working process of the RL, we need to consider two main things: </p>
    <ul class="points">
    <li><strong>Environment:</strong> It can be anything such as a room, maze, football ground, etc. </li>
    <li><strong>Agent:</strong> An intelligent agent such as AI robot.</li>
    </ul>
    <p>Let's take an example of a maze environment that the agent needs to explore. Consider the below image:</p>
    <img src="https://static.javatpoint.com/tutorial/reinforcement-learning/images/reinforcement-learning-works.png" alt="How does Reinforcement Learning Works">
    <p>In the above image, the agent is at the very first block of the maze. The maze is consisting of an S<sub>6</sub> block, which is a <strong>wall</strong>, S<sub>8</sub> a <strong>fire pit</strong>, and S<sub>4</sub> a <strong>diamond block.</strong></p>
    <p>The agent cannot cross the S<sub>6</sub> block, as it is a solid wall. If the agent reaches the S<sub>4</sub> block, then get the <strong>+1 reward; </strong>if it reaches the fire pit, then gets <strong>-1 reward point</strong>. It can take four actions<strong>: move up, move down, move left, and move right.</strong></p>
    <p>The agent can take any path to reach to the final point, but he needs to make it in possible fewer steps. Suppose the agent considers the path <strong>S9-S5-S1-S2-S3</strong>, so he will get the +1-reward point.</p>
    <p>The agent will try to remember the preceding steps that it has taken to reach the final step. To memorize the steps, it assigns 1 value to each previous step. Consider the below step:</p>
    <img src="https://static.javatpoint.com/tutorial/reinforcement-learning/images/reinforcement-learning-works2.png" alt="How does Reinforcement Learning Works">
    <p>Now, the agent has successfully stored the previous steps assigning the 1 value to each previous block. But what will the agent do if he starts moving from the block, which has 1 value block on both sides? Consider the below diagram:</p>
    <img src="https://static.javatpoint.com/tutorial/reinforcement-learning/images/reinforcement-learning-works3.png" alt="How does Reinforcement Learning Works">
    <p>It will be a difficult condition for the agent whether he should go up or down as each block has the same value. So, the above approach is not suitable for the agent to reach the destination. Hence to solve the problem, we will use the <strong>Bellman equation</strong>, which is the main concept behind reinforcement learning. </p>
    <hr>
    <div id="c77b05e7-25e0-457b-93ce-bea8b5cdc41b" data-section="c77b05e7-25e0-457b-93ce-bea8b5cdc41b" class="_ap_apex_ad" data-xpath="#city > table:eq(0) > tbody:eq(0) > tr:eq(0) > td:eq(0) > h2:eq(6)" data-section-id="" data-ap-network="adpTags" data-render-time="1700496564706" data-refresh-time="1700496669719" data-timeout="7356" style="display: block; clear: both; text-align: center; margin: 10px auto; width: 0px; height: 0px; overflow: hidden; visibility: hidden;"><div id="ADP_37780_728X280_c77b05e7-25e0-457b-93ce-bea8b5cdc41b" style="text-align: center; margin: 0 auto;" data-google-query-id="CMWUn-H70oIDFcUE1QodEZYK6Q">
    <script type="text/javascript">
    window.adpushup.adpTags.que.push(function(){
    window.adpushup.adpTags.display("ADP_37780_728X280_c77b05e7-25e0-457b-93ce-bea8b5cdc41b");
    });
    </script>
    <div id="google_ads_iframe_/103512698,22511567001/22903275080_0__container__" style="border: 0pt none; width: 728px; height: 0px;"></div></div></div><h2 id="Bellman" class="h2">The Bellman Equation</h2>
    <p>The Bellman equation was introduced by the Mathematician <strong>Richard Ernest Bellman in the year 1953</strong>, and hence it is called as a Bellman equation. It is associated with dynamic programming and used to calculate the values of a decision problem at a certain point by including the values of previous states. </p>
    <p>It is a way of calculating the value functions in dynamic programming or environment that leads to modern reinforcement learning. </p>
    <p>The key-elements used in Bellman equations are:</p>
    <ul class="points">
    <li>Action performed by the agent is referred to as "a"</li>
    <li>State occurred by performing the action is "s."</li>
    <li>The reward/feedback obtained for each good and bad action is "R."</li>
    <li>A discount factor is Gamma "γ." </li>
    </ul>
    <p>The Bellman equation can be written as:</p>
    <div class="codeblock"><textarea name="code" class="java">V(s) = max [R(s,a) + γV(s`)]
    </textarea></div>
    <p>Where,</p>
    <p><strong>V(s)= value calculated at a particular point.</strong></p>
    <p><strong>R(s,a) = Reward at a particular state s by performing an action.</strong></p>
    <p><strong>γ = Discount factor</strong></p>
    <p><strong>V(s`) = The value at the previous state.</strong></p>
    <p>In the above equation, we are taking the max of the complete values because the agent tries to find the optimal solution always.</p>
    <p>So now, using the Bellman equation, we will find value at each state of the given environment. We will start from the block, which is next to the target block.</p>
    <p class="pq"><strong>For 1st block:</strong></p>
    <p>V(s3) = max [R(s,a) + γV(s`)], here V(s')= 0 because there is no further state to move.</p>
    <p>V(s3)= max[R(s,a)]=&gt; V(s3)= max[1]=&gt; <strong>V(s3)= 1.</strong></p>
    <p class="pq"><strong>For 2nd block: </strong></p>
    <p>V(s2) = max [R(s,a) + γV(s`)], here γ= 0.9(lets), V(s')= 1, and R(s, a)= 0, because there is no reward at this state.</p>
    <p>V(s2)= max[0.9(1)]=&gt; V(s)= max[0.9]=&gt; <strong>V(s2) =0.9</strong></p>
    <p class="pq"><strong>For 3rd block:</strong></p>
    <p>V(s1) = max [R(s,a) + γV(s`)], here γ= 0.9(lets), V(s')= 0.9, and R(s, a)= 0, because there is no reward at this state also.</p>
    <p>V(s1)= max[0.9(0.9)]=&gt; V(s3)= max[0.81]=&gt; <strong>V(s1) =0.81</strong></p>
    <p class="pq"><strong>For 4th block:</strong></p>
    <p>V(s5) = max [R(s,a) + γV(s`)], here γ= 0.9(lets), V(s')= 0.81, and R(s, a)= 0, because there is no reward at this state also.</p>
    <p>V(s5)= max[0.9(0.81)]=&gt; V(s5)= max[0.81]=&gt; <strong>V(s5) =0.73</strong></p>
    <p class="pq"><strong>For 5th block:</strong></p>
    <p>V(s9) = max [R(s,a) + γV(s`)], here γ= 0.9(lets), V(s')= 0.73, and R(s, a)= 0, because there is no reward at this state also.</p>
    <p>V(s9)= max[0.9(0.73)]=&gt; V(s4)= max[0.81]=&gt; <strong>V(s4) =0.66</strong></p>
    <p><strong>Consider the below image:</strong></p>
    <img src="https://static.javatpoint.com/tutorial/reinforcement-learning/images/reinforcement-learning-bellman-equation.png" alt="Bellman Equation">
    <p>Now, we will move further to the 6<sup>th</sup> block, and here agent may change the route because it always tries to find the optimal path. So now, let's consider from the block next to the fire pit.</p>
    <img src="https://static.javatpoint.com/tutorial/reinforcement-learning/images/reinforcement-learning-bellman-equation2.png" alt="Bellman Equation">
    <p>Now, the agent has three options to move; if he moves to the blue box, then he will feel a bump if he moves to the fire pit, then he will get the -1 reward. But here we are taking only positive rewards, so for this, he will move to upwards only. The complete block values will be calculated using this formula. Consider the below image:</p>
    <img src="https://static.javatpoint.com/tutorial/reinforcement-learning/images/reinforcement-learning-bellman-equation3.png" alt="Bellman Equation">
    <hr>
    <div id="cbe30954-5f7c-4b2e-b82f-765cad340ca7" data-section="cbe30954-5f7c-4b2e-b82f-765cad340ca7" class="_ap_apex_ad" data-xpath="#city > table:eq(0) > tbody:eq(0) > tr:eq(0) > td:eq(0) > h2:eq(7)" data-section-id="" data-ap-network="adpTags" data-render-time="1700496566736" data-refresh-time="1700496585964" data-timeout="2284" style="display: block; clear: both; text-align: center; margin: 10px auto; width: 0px; height: 0px; overflow: hidden; visibility: hidden;"><div id="ADP_37780_728X250_cbe30954-5f7c-4b2e-b82f-765cad340ca7" style="text-align: center; margin: 0 auto;" data-google-query-id="COOhvq_70oIDFVoV1QodK6oBnA">
    <script type="text/javascript">
    window.adpushup.adpTags.que.push(function(){
    window.adpushup.adpTags.display("ADP_37780_728X250_cbe30954-5f7c-4b2e-b82f-765cad340ca7");
    });
    </script>
    <div id="google_ads_iframe_/103512698,22511567001/22912473830_0__container__" style="border: 0pt none; width: 728px; height: 0px;"></div></div></div><h2 id="Types" class="h2">Types of Reinforcement learning</h2>
    <p>There are mainly two types of reinforcement learning, which are:</p>
    <ul class="points">
    <li><strong>Positive Reinforcement</strong></li>
    <li><strong>Negative Reinforcement</strong></li>
    </ul>
    <p class="pq"><strong>Positive Reinforcement:</strong></p>
    <p>The positive reinforcement learning means adding something to increase the tendency that expected behavior would occur again. It impacts positively on the behavior of the agent and increases the strength of the behavior.</p>
    <p>This type of reinforcement can sustain the changes for a long time, but too much positive reinforcement may lead to an overload of states that can reduce the consequences.</p>
    <p class="pq"><strong>Negative Reinforcement:</strong></p>
    <p>The negative reinforcement learning is opposite to the positive reinforcement as it increases the tendency that the specific behavior will occur again by avoiding the negative condition.</p>
    <p>It can be more effective than the positive reinforcement depending on situation and behavior, but it provides reinforcement only to meet minimum behavior.</p>
    <h3 class="h3">How to represent the agent state?</h3>
    <p>We can represent the agent state using the <strong>Markov State</strong> that contains all the required information from the history. The State St is Markov state if it follows the given condition:</p>
    <div class="codeblock"><pre>P[S<sub>t</sub>+1 | S<sub>t </sub>] = P[S<sub>t </sub>+1 | S<sub>1</sub>,......, S<sub>t</sub>]
    </pre></div>
    <p></p><div id="f49e589f-fbc5-463d-999f-47c781c68759" data-section="f49e589f-fbc5-463d-999f-47c781c68759" class="_ap_apex_ad" data-xpath="  #city > table:eq(0) > tbody:eq(0) > tr:eq(0) > td:eq(0) > p:eq(54)  " data-section-id="" data-ap-network="adpTags" data-render-time="1700496566779" data-refresh-time="1700496587739" data-timeout="2445" style="display: block; clear: both; text-align: center; margin: 10px auto; width: 0px; height: 0px; overflow: hidden; visibility: hidden;"><div id="ADP_37780_728X280_f49e589f-fbc5-463d-999f-47c781c68759" style="text-align: center; margin: 0 auto;" data-google-query-id="CMbpwK_70oIDFdiTrAIdAroN9Q">
    <script type="text/javascript">
    window.adpushup.adpTags.que.push(function(){
    window.adpushup.adpTags.display("ADP_37780_728X280_f49e589f-fbc5-463d-999f-47c781c68759");
    });
    </script>
    <div id="google_ads_iframe_/103512698,22511567001/22794806204_0__container__" style="border: 0pt none; width: 728px; height: 0px;"></div></div></div>The Markov state follows the <strong>Markov property</strong>, which says that the future is independent of the past and can only be defined with the present. The RL works on fully observable environments, where the agent can observe the environment and act for the new state. The complete process is known as Markov Decision process, which is explained below:<p></p>
    <hr>
    <h2 id="Markov" class="h2">Markov Decision Process</h2>
    <p>Markov Decision Process or MDP, is used to <strong>formalize the reinforcement learning problems</strong>. If the environment is completely observable, then its dynamic can be modeled as a <strong>Markov Process</strong>. In MDP, the agent constantly interacts with the environment and performs actions; at each action, the environment responds and generates a new state.</p>
    <img src="https://static.javatpoint.com/tutorial/reinforcement-learning/images/reinforcement-learning-markov-decision-process.png" alt="Markov Decision Process">
    <p>MDP is used to describe the environment for the RL, and almost all the RL problem can be formalized using MDP. </p>
    <p>MDP contains a tuple of four elements (S, A, P<sub>a</sub>, R<sub>a</sub>):</p>
    <ul class="points">
    <li>A set of finite States S</li>
    <li>A set of finite Actions A</li>
    <li>Rewards received after transitioning from state S to state S', due to action a.</li>
    <li>Probability P<sub>a</sub>.</li>
    </ul>
    <p>MDP uses <strong>Markov property</strong>, and to better understand the MDP, we need to learn about it.</p>
    <h3 class="h3">Markov Property:</h3>
    <p>It says that <strong><em>"If the agent is present in the current state S1, performs an action a1 and move to the state s2, then the state transition from s1 to s2 only depends on the current state and future action and states do not depend on past actions, rewards, or states."</em></strong></p><div id="b149beca-dba0-4e76-b01a-09a32442414c" data-section="b149beca-dba0-4e76-b01a-09a32442414c" class="_ap_apex_ad" data-xpath="#city > table:eq(0) > tbody:eq(0) > tr:eq(0) > td:eq(0) > p:eq(59)" data-section-id="" data-ap-network="adpTags" data-render-time="1700496572095" data-refresh-time="1700496589229" data-timeout="2526" style="display: block; clear: both; text-align: center; margin: 10px auto; width: 0px; height: 0px; overflow: hidden; visibility: hidden;"><div id="ADP_37780_728X280_b149beca-dba0-4e76-b01a-09a32442414c" style="text-align: center; margin: 0 auto;" data-google-query-id="CLLIg7L70oIDFZEU1Qodj1wPaA">
    <script type="text/javascript">
    window.adpushup.adpTags.que.push(function(){
    window.adpushup.adpTags.display("ADP_37780_728X280_b149beca-dba0-4e76-b01a-09a32442414c");
    });
    </script>
    <div id="google_ads_iframe_/103512698,22511567001/22987728091_0__container__" style="border: 0pt none; width: 728px; height: 0px;"></div></div></div><p></p>
    <p>Or, in other words, as per Markov Property, the current state transition does not depend on any past action or state. Hence, MDP is an RL problem that satisfies the Markov property. Such as in a <strong>Chess game, the players only focus on the current state and do not need to remember past actions or states</strong>.</p>
    <p class="pq"><strong>Finite MDP:</strong></p>
    <p>A finite MDP is when there are finite states, finite rewards, and finite actions. In RL, we consider only the finite MDP. </p>
    <h3 class="h3">Markov Process:</h3>
    <p>Markov Process is a memoryless process with a sequence of random states S<sub>1</sub>, S<sub>2</sub>, ....., S<sub>t</sub> that uses the Markov Property. Markov process is also known as Markov chain, which is a tuple (S, P) on state S and transition function P. These two components (S and P) can define the dynamics of the system. </p>
    <hr>
    <h2 id="Algorithm" class="h2">Reinforcement Learning Algorithms</h2>
    <p>Reinforcement learning algorithms are mainly used in AI applications and gaming applications. The main used algorithms are:</p>
    <ul class="points">
    <li><strong>Q-Learning:</strong>
    <ul class="points">
    <li>Q-learning is an <strong>Off policy RL algorithm</strong>, which is used for the temporal difference Learning. The temporal difference learning methods are the way of comparing temporally successive predictions. </li>
    <li>It learns the value function Q (S, a), which means how good to take action "<strong>a</strong>" at a particular state "<strong>s</strong>." </li>
    <li>The below flowchart explains the working of Q- learning:</li>
    </ul></li>
    </ul>
    <img src="https://static.javatpoint.com/tutorial/reinforcement-learning/images/reinforcement-learning-algorithms.png" alt="Reinforcement Learning Algorithms">
    <ul class="points">
    <li><strong>State Action Reward State action (SARSA):</strong>
    <ul class="points">
    <li>SARSA stands for <strong>State Action Reward State action</strong>, which is an <strong>on-policy</strong> temporal difference learning method. The on-policy control method selects the action for each state while learning using a specific policy.</li>
    <li>The goal of SARSA is to calculate the <strong>Q π (s, a) for the selected current policy π and all pairs of (s-a). </strong></li>
    <li>The main difference between Q-learning and SARSA algorithms is that <strong>unlike Q-learning, the </strong></li></ul></li>
     <h3 class="h3">Feedback</h3>
    <ul class="points">
    <li>Send your Feedback to nigammishra826@gmail.com</li>
    </ul></i></b>
  </ul></td></tr></tbody></table>
  <head>
    <script src="https://kit.fontawesome.com/3b161c540c.js" crossorigin="anonymous"></script>
  </head>
  
  <body>
    <footer class="footer">
      <div class="container row">
        <div class="footer-col">
          <h4>About</h4>
          <ul>
            <li><a href="nigam1.html">Artificial intelligence(AI)</a></li>
            <li><a href="nigam4.html">Types of AI</a></li>
            <li><a href="nigam8.html">Turing test of AI</a></li>
            <li><a href="nigam22.html">The Wumpus world</a></li>
          </ul>
        </div>
        <div class="footer-col">
          <h4>get help</h4>
          <ul>
            <li><a href="nigam89.html">MCQ</a></li>
            <li><a href="nigam87.html">Machine Learning</a></li>
            <li><a href="nigam88.html">NPL</a></li>
            <li><a href="nigam92.html">Data Science</a></li>
            <li><a href="nigam93.html">Reinforcement learning</a></li>
          </ul>
        </div>
        <div class="footer-col">
          <h4>Misc</h4>
          <ul>
            <li><a href="nigam36.html">examples</a></li>
            <li><a href="nigam43.html">Robotic & AI</a></li>
            <li><a href="nigam44.html">Future of AI</a></li>
            <li><a href="nigam47.html">Scope of AI</a></li>
            <li><a href="nigam81.html">open AI&ChatGPT</a></li>
          </ul>
        </div>
        <div class="footer-col">
          <h4>follow us</h4>
          <div class="social-links">
            <a href="https://www.facebook.com/nigam.mishra.710?mibextid=9R9pXO"><i class="fa-brands fa-facebook-f"></i></a>
            <a href="https://twitter.com/MishraNugam?s=09"><i class="fa-brands fa-twitter"></i></a>
            <a href="https://instagram.com/mr_nigam_8199?igshid=MXNzeGI3bmZ6b3M3NQ=="><i class="fa-brands fa-instagram"></i></a>
            <a href=""><i class="fa-brands fa-youtube"></i></a>
          </div>
          <a href="contact1.html"><img src="photo/logo2.jpg" style="height: 100px;width: 100px;"></a>
        </div>
      </div>
    </footer>
  </body>
</div></div></div></i></b> 
    
  </body></html>